{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading large line-by-line JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS_JSON_FILE = 'geotagged_tweets_20160812-0912.jsons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since file size of the tweets is large, pandas needs to read the file in chunks.\n",
    "chunked_reader = pd.read_json(TWEETS_JSON_FILE, \n",
    "                                lines=True, \n",
    "                                orient='records', \n",
    "                                chunksize=50000)\n",
    "chunked_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dataframe on which the chunks can be appended\n",
    "df_tweets_raw = pd.DataFrame()\n",
    "\n",
    "# Append chunks to dataframe\n",
    "i = 1\n",
    "for chunk in chunked_reader:\n",
    "    df_tweets_raw = df_tweets_raw.append(chunk)\n",
    "    print(i,'chunk read')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unnest columns\n",
    "As the original JSON has multi-level nesting, and pandas only reads the first level, we need to do the rest ourselves..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnest_column(df, prefix):\n",
    "    '''Creates separate columns in the dataframe from a single column containing a dict. \n",
    "        Returns a dataframe.\n",
    "    '''\n",
    "    \n",
    "    # get the fields from inside the nested column (== keys of dict)\n",
    "    keys = list(df.iloc[0][prefix].keys())\n",
    "    \n",
    "    # Initialize the new unnested columns\n",
    "    for key in keys:\n",
    "        df[ prefix + '_' + key ] = np.nan\n",
    "    print('initialized empty columns for', prefix)\n",
    "        \n",
    "    # Fill the new columns by apply'ing\n",
    "    for key in keys:\n",
    "        print('unnesting', key,'from',prefix)\n",
    "        df[prefix+'_'+key] = df[prefix].apply(lambda x: x[key] if type(x) == type(dict()) and key in x else np.nan)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnest the place columns into place_*\n",
    "df_tweets_raw = unnest_column(df_tweets_raw, 'place')\n",
    "df_tweets_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:uva-ds-fods-ass2]",
   "language": "python",
   "name": "conda-env-uva-ds-fods-ass2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
